{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5gv2oOVyAZEVJHOW1T8YG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fmhirwa/ML-Pipeline_Task1/blob/main/APIs_and_Web_scraping_Peer_Learning_Activity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 0"
      ],
      "metadata": {
        "id": "9ExVR4YYXb-O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iu9LPULRUds_",
        "outputId": "82275ff9-37bf-4d02-a2c1-711fa89faaba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['CR90 corvette', 'Sentinel-class landing craft', 'Death Star', 'Executor', 'Rebel transport', 'Imperial shuttle', 'EF76 Nebulon-B escort frigate', 'Calamari Cruiser', 'Republic Cruiser', 'Droid control ship', 'J-type diplomatic barge', 'AA-9 Coruscant freighter', 'Republic Assault ship', 'Solar Sailer', 'Trade Federation cruiser', 'Theta-class T-2c shuttle', 'Republic attack cruiser']\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"Code to return the list of ships\"\"\"\n",
        "\n",
        "import requests\n",
        "\n",
        "def availableShips(passengerCount):\n",
        "    \"\"\" List of ships\n",
        "\n",
        "    Arguments:\n",
        "        passengerCount (int): number of passengers\n",
        "    \"\"\"\n",
        "    res = requests.get('https://swapi-api.alx-tools.com/api/starships')\n",
        "\n",
        "    output = []\n",
        "    while res.status_code == 200:\n",
        "        res = res.json()\n",
        "        for ship in res['results']:\n",
        "            passengers = ship['passengers'].replace(',', '')\n",
        "            try:\n",
        "                if int(passengers) >= passengerCount:\n",
        "                    output.append(ship['name'])\n",
        "            except ValueError:\n",
        "                pass\n",
        "        if res['next']:\n",
        "            res = requests.get(res['next'])\n",
        "        else:\n",
        "            break\n",
        "    return output\n",
        "\n",
        "# Testing; For colab only\n",
        "# Get ships with at least 10 passengers\n",
        "ships = availableShips(10)\n",
        "print(ships)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scrapping"
      ],
      "metadata": {
        "id": "BJ9y3k7VXeHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4 pandas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgXfarCVXjCY",
        "outputId": "7451ff7a-e8f8-4231-d581-02eb7475d146"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# URL to scrape data from\n",
        "url = 'https://www.scrapethissite.com/pages/forms/'\n",
        "\n",
        "# Send a GET request to fetch the page content\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parse the HTML content using BeautifulSoup\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Find the table containing data\n",
        "table = soup.find('table')\n",
        "\n",
        "# Extract headers\n",
        "headers = [header.text.strip() for header in table.find_all('th')]\n",
        "\n",
        "# Extract rows\n",
        "rows = []\n",
        "for row in table.find_all('tr')[1:]:\n",
        "    cols = row.find_all('td')\n",
        "    data = [col.text.strip() for col in cols]\n",
        "    rows.append(data)\n",
        "\n",
        "# Create a DataFrame using the headers and rows\n",
        "df = pd.DataFrame(rows, columns=headers)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('scraped_data.csv', index=False)\n",
        "\n",
        "# Display the DataFrame in the output\n",
        "print(\"Scraped Data:\")\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ruSosK9Xm1i",
        "outputId": "5e1de2d5-6f3e-4060-845c-864cc76741f8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped Data:\n",
            "                Team Name  Year Wins Losses OT Losses  Win % Goals For (GF)  \\\n",
            "0           Boston Bruins  1990   44     24             0.55            299   \n",
            "1          Buffalo Sabres  1990   31     30            0.388            292   \n",
            "2          Calgary Flames  1990   46     26            0.575            344   \n",
            "3      Chicago Blackhawks  1990   49     23            0.613            284   \n",
            "4       Detroit Red Wings  1990   34     38            0.425            273   \n",
            "5         Edmonton Oilers  1990   37     37            0.463            272   \n",
            "6        Hartford Whalers  1990   31     38            0.388            238   \n",
            "7       Los Angeles Kings  1990   46     24            0.575            340   \n",
            "8   Minnesota North Stars  1990   27     39            0.338            256   \n",
            "9      Montreal Canadiens  1990   39     30            0.487            273   \n",
            "10      New Jersey Devils  1990   32     33              0.4            272   \n",
            "11     New York Islanders  1990   25     45            0.312            223   \n",
            "12       New York Rangers  1990   36     31             0.45            297   \n",
            "13    Philadelphia Flyers  1990   33     37            0.412            252   \n",
            "14    Pittsburgh Penguins  1990   41     33            0.512            342   \n",
            "15       Quebec Nordiques  1990   16     50              0.2            236   \n",
            "16        St. Louis Blues  1990   47     22            0.588            310   \n",
            "17    Toronto Maple Leafs  1990   23     46            0.287            241   \n",
            "18      Vancouver Canucks  1990   28     43             0.35            243   \n",
            "19    Washington Capitals  1990   37     36            0.463            258   \n",
            "20          Winnipeg Jets  1990   26     43            0.325            260   \n",
            "21          Boston Bruins  1991   36     32             0.45            270   \n",
            "22         Buffalo Sabres  1991   31     37            0.388            289   \n",
            "23         Calgary Flames  1991   31     37            0.388            296   \n",
            "24     Chicago Blackhawks  1991   36     29             0.45            257   \n",
            "\n",
            "   Goals Against (GA) + / -  \n",
            "0                 264    35  \n",
            "1                 278    14  \n",
            "2                 263    81  \n",
            "3                 211    73  \n",
            "4                 298   -25  \n",
            "5                 272     0  \n",
            "6                 276   -38  \n",
            "7                 254    86  \n",
            "8                 266   -10  \n",
            "9                 249    24  \n",
            "10                264     8  \n",
            "11                290   -67  \n",
            "12                265    32  \n",
            "13                267   -15  \n",
            "14                305    37  \n",
            "15                354  -118  \n",
            "16                250    60  \n",
            "17                318   -77  \n",
            "18                315   -72  \n",
            "19                258     0  \n",
            "20                288   -28  \n",
            "21                275    -5  \n",
            "22                299   -10  \n",
            "23                305    -9  \n",
            "24                236    21  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Amazon Scrapping"
      ],
      "metadata": {
        "id": "2t1esZ7yh2pS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Florent: Amazon scrapping; Amazon blocks after repeated use"
      ],
      "metadata": {
        "id": "f874zFZ8kxzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "\n",
        "# Create a directory to save images\n",
        "os.makedirs('product_images', exist_ok=True)\n",
        "\n",
        "# Function to scrape products\n",
        "def scrape_products(search_query):\n",
        "    # Replace spaces with '+' for the search query\n",
        "    search_query = search_query.replace(' ', '+')\n",
        "    url = f'https://www.amazon.com/s?k={search_query}'\n",
        "\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:92.0) Gecko/20100101 Firefox/92.0',\n",
        "        'Accept-Language': 'en-US, en;q=0.9',\n",
        "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',\n",
        "        'Referer': 'https://www.amazon.com/',\n",
        "    }\n",
        "\n",
        "    # Send a GET request to fetch the page content\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    # Parse the HTML content using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    products = []\n",
        "    count = 0\n",
        "\n",
        "    # Loop through search results and extract product data\n",
        "    for product in soup.find_all('div', {'data-component-type': 's-search-result'}):\n",
        "        # Extract product name\n",
        "        name = product.h2.text.strip()\n",
        "\n",
        "        # Extract image URL\n",
        "        image_tag = product.find('img', {'class': 's-image'})\n",
        "        image_url = image_tag['src'] if image_tag else None\n",
        "\n",
        "        if name and image_url and count < 5:\n",
        "            # Save image locally\n",
        "            image_response = requests.get(image_url)\n",
        "            image_path = f'product_images/{name[:30].replace(\" \", \"_\")}.jpg'\n",
        "            with open(image_path, 'wb') as f:\n",
        "                f.write(image_response.content)\n",
        "\n",
        "            products.append((name, image_path))\n",
        "            count += 1\n",
        "\n",
        "    return products\n",
        "\n",
        "# Searching for \"cookies\"\n",
        "products = scrape_products(\"laptops\")\n",
        "\n",
        "# Display product names and image paths\n",
        "print(\"Scraped Products:\")\n",
        "for product_name, image_path in products:\n",
        "    print(f'Product: {product_name}, Image saved at: {image_path}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_T8iI8uAX_et",
        "outputId": "b3ad2216-05d9-4c90-850a-e50a7a1b479a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped Products:\n",
            "Product: Acer Aspire 3 A315-24P-R7VH Slim Laptop | 15.6\" Full HD IPS Display | AMD Ryzen 3 7320U Quad-Core Processor | AMD Radeon Graphics | 8GB LPDDR5 | 128GB NVMe SSD | Wi-Fi 6 | Windows 11 Home in S Mode, Image saved at: product_images/Acer_Aspire_3_A315-24P-R7VH_Sl.jpg\n",
            "Product: HP Newest 255 G10 Laptop for Home or Work, 16GB RAM, 1TB SSD, 15.6\" Full HD, Ryzen 3 7330U (Beat Intel i5-1135G7), Ethernet Port, HDMI, USB-C, Windows 11 Pro, Business and Fun Ready (2024), Image saved at: product_images/HP_Newest_255_G10_Laptop_for_H.jpg\n",
            "Product: HP Newest 14\" Ultral Light Laptop for Students and Business, Intel Quad-Core N4120, 8GB RAM, 192GB Storage(64GB eMMC+128GB Micro SD), 1 Year Office 365, Webcam, HDMI, WiFi, USB-A&C, Win 11 S, Image saved at: product_images/HP_Newest_14\"_Ultral_Light_Lap.jpg\n",
            "Product: Acer Aspire Go 15 Slim Laptop | 15.6\" Full HD IPS 1080P Display | Intel Core i3-N305| Intel UHD Graphics | 8GB LPDDR5 | 128GB HD | Wi-Fi 6 | AI PC | Windows 11 Home in S Mode | AG15-31P-3947, Image saved at: product_images/Acer_Aspire_Go_15_Slim_Laptop_.jpg\n",
            "Product: HP Portable Laptop, Student and Business, 14\" HD Display, Intel Quad-Core N4120, 8GB DDR4 RAM, 64GB eMMC, 1 Year Office 365, Webcam, RJ-45, HDMI, Wi-Fi, Windows 11 Home, Silver, Image saved at: product_images/HP_Portable_Laptop,_Student_an.jpg\n"
          ]
        }
      ]
    }
  ]
}